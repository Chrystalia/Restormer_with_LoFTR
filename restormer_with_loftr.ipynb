{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: imutils in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opencv-python) (1.23.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.23.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.23.2)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (9.2.0)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (2.31.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=21->scikit-image) (3.0.9)\n",
      "Requirement already satisfied: natsort in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (8.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.23.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install einops\n",
    "! pip install imutils\n",
    "! pip install opencv-python\n",
    "! pip install matplotlib\n",
    "! pip install scikit-image\n",
    "! pip install natsort\n",
    "! pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import namedtuple\n",
    "import cv2\n",
    "import csv\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from kornia.feature.loftr import LoFTR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "import imutils\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from kornia_moons.feature import *\n",
    "\n",
    "# from src.loftr import LoFTR, default_cfg\n",
    "from src.utils.plotting import make_matching_figure\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# =============================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from runpy import run_path\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "from numpy.lib.arraypad import pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "matcher = KF.LoFTR(pretrained=None)\n",
    "matcher.load_state_dict(torch.load(\"weights/outdoor_ds.ckpt\")['state_dict'])\n",
    "matcher = matcher.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LONGEST_EDGE = 1280\n",
    "def resize_keep_ratio(img, longest_size=LONGEST_EDGE):\n",
    "    height, width = img.shape[:2]\n",
    "    if np.maximum(height, width) <= longest_size: # no need to resize\n",
    "        return img\n",
    "    \n",
    "    if height >= width:\n",
    "        resized_img = imutils.resize(img, height=longest_size)\n",
    "    else:\n",
    "        resized_img = imutils.resize(img, width=longest_size)\n",
    "    return resized_img\n",
    "\n",
    "def load_torch_image(fname):\n",
    "    img = cv2.imread(fname)\n",
    "    scale = 840 / max(img.shape[0], img.shape[1]) \n",
    "    w = int(img.shape[1] * scale)\n",
    "    h = int(img.shape[0] * scale)\n",
    "    img = cv2.resize(img, (w, h))\n",
    "\n",
    "    # img = resize_keep_ratio(img)\n",
    "    # img = cv2.resize(img, (img.shape[1]//8*8, img.shape[0]//8*8))  # input size should be divisible by 8\n",
    "\n",
    "    img = K.image_to_tensor(img, False).float() /255.\n",
    "    img = K.color.bgr_to_rgb(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(img_path0, img_path1, matcher, device=device):\n",
    "    img0 = load_torch_image(img_path0)\n",
    "    img1 = load_torch_image(img_path1)\n",
    "        \n",
    "    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0).to(device), \n",
    "                  \"image1\": K.color.rgb_to_grayscale(img1).to(device)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correspondences = matcher(input_dict)\n",
    "        \n",
    "    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n",
    "    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n",
    "        \n",
    "    return mkpts0, mkpts1\n",
    "        \n",
    "def get_F_matrix(mkpts0, mkpts1):\n",
    "\n",
    "    # Make sure we do not trigger an exception here.\n",
    "    if len(mkpts0) > 8:\n",
    "        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\n",
    "\n",
    "        assert F.shape == (3, 3), 'Malformed F?'\n",
    "    else:\n",
    "        F = np.zeros((3, 3))\n",
    "\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling factors: {'british_museum': 2.517, 'brandenburg_gate': 7.38, 'buckingham_palace': 18.75, 'colosseum_exterior': 36.99, 'grand_place_brussels': 10.26, 'lincoln_memorial_statue': 1.85, 'notre_dame_front_facade': 1.36, 'pantheon_exterior': 5.41, 'piazza_san_marco': 7.92, 'sacre_coeur': 20.27, 'sagrada_familia': 4.2, 'st_pauls_cathedral': 7.01, 'st_peters_square': 21.48, 'taj_mahal': 20.76, 'temple_nara_japan': 7.79, 'trevi_fountain': 3.67}\n"
     ]
    }
   ],
   "source": [
    "src = '../image-matching-challenge-2022/train'\n",
    "\n",
    "scaling_dict = {}\n",
    "with open(f'{src}/scaling_factors.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for i, row in enumerate(reader):\n",
    "        # Skip header.\n",
    "        if i == 0:\n",
    "            continue\n",
    "        scaling_dict[row[0]] = float(row[1])\n",
    "\n",
    "print(f'Scaling factors: {scaling_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restoration(img_pth0, img_pth1):\n",
    "    # task = 'Real_Denoising'\n",
    "    task = 'Single_Image_Defocus_Deblurring'\n",
    "    # task = 'Motion_Deblurring'\n",
    "    # task = 'Deraining'\n",
    "\n",
    "    shutil.rmtree('demo')\n",
    "\n",
    "    input_dir = 'demo/sample_images/'+task+'/degraded'\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    \n",
    "    shutil.copy(img_pth0, input_dir)\n",
    "    shutil.copy(img_pth1, input_dir)\n",
    "\n",
    "    def get_weights_and_parameters(task, parameters):\n",
    "        if task == 'Motion_Deblurring':\n",
    "            weights = os.path.join('Motion_Deblurring', 'pretrained_models', 'motion_deblurring.pth')\n",
    "        elif task == 'Single_Image_Defocus_Deblurring':\n",
    "            weights = os.path.join('Defocus_Deblurring', 'pretrained_models', 'single_image_defocus_deblurring.pth')\n",
    "        elif task == 'Deraining':\n",
    "            weights = os.path.join('Deraining', 'pretrained_models', 'deraining.pth')\n",
    "        elif task == 'Real_Denoising':\n",
    "            weights = os.path.join('Denoising', 'pretrained_models', 'real_denoising.pth')\n",
    "            parameters['LayerNorm_type'] =  'BiasFree'\n",
    "        return weights, parameters\n",
    "\n",
    "    # Get model weights and parameters\n",
    "    parameters = {'inp_channels':3, 'out_channels':3, 'dim':48, 'num_blocks':[4,6,6,8], 'num_refinement_blocks':4, 'heads':[1,2,4,8], 'ffn_expansion_factor':2.66, 'bias':False, 'LayerNorm_type':'WithBias', 'dual_pixel_task':False}\n",
    "    weights, parameters = get_weights_and_parameters(task, parameters)\n",
    "\n",
    "    load_arch = run_path(os.path.join('basicsr', 'models', 'archs', 'restormer_arch.py'))\n",
    "    model = load_arch['Restormer'](**parameters)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "    # print(device)\n",
    "    checkpoint = torch.load(weights)\n",
    "    # print(checkpoint)\n",
    "    model.load_state_dict(checkpoint['params'])\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    input_dir = 'demo/sample_images/'+task+'/degraded'\n",
    "    out_dir = 'demo/sample_images/'+task+'/restored'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    extensions = ['jpg', 'JPG', 'png', 'PNG', 'jpeg', 'JPEG', 'bmp', 'BMP']\n",
    "    files = natsorted(glob(os.path.join(input_dir, '*')))\n",
    "\n",
    "    img_multiple_of = 8\n",
    "\n",
    "    print(f\"\\n ==> Running {task} with weights {weights}\\n \")\n",
    "    with torch.no_grad():\n",
    "        for filepath in tqdm(files):\n",
    "            img = cv2.cvtColor(cv2.imread(filepath), cv2.COLOR_BGR2RGB)\n",
    "            input_ = torch.from_numpy(img).float().div(255.).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "            # Pad the input if not_multiple_of 8\n",
    "            h, w = input_.shape[2], input_.shape[3]\n",
    "            H, W = ((h + img_multiple_of) // img_multiple_of) * img_multiple_of, ((w + img_multiple_of) // img_multiple_of) * img_multiple_of\n",
    "            padh = H - h if h % img_multiple_of != 0 else 0\n",
    "            padw = W - w if w % img_multiple_of != 0 else 0\n",
    "\n",
    "            # Convert input_ to NumPy array\n",
    "            input_ = input_.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "            input_ = np.pad(input_, ((0, padh), (0, padw), (0, 0)), mode='reflect')\n",
    "            input_ = torch.from_numpy(input_).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "            restored = model(input_)\n",
    "            restored = torch.clamp(restored, 0, 1)\n",
    "\n",
    "            # Unpad the output\n",
    "            restored = restored[:, :, :h, :w]\n",
    "\n",
    "            restored = restored.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "            restored = img_as_ubyte(restored[0])\n",
    "\n",
    "            filename = os.path.split(filepath)[-1]\n",
    "            cv2.imwrite(os.path.join(out_dir, filename), cv2.cvtColor(restored, cv2.COLOR_RGB2BGR))    \n",
    "    \n",
    "    new_img_pth0 = out_dir + '/' + os.listdir(out_dir)[0]\n",
    "    new_img_pth1 = out_dir + '/' + os.listdir(out_dir)[1]\n",
    "    print(os.listdir(out_dir)[0] + ' RESTORED')\n",
    "    print(os.listdir(out_dir)[1] + ' RESTORED')\n",
    "    return new_img_pth0, new_img_pth1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restoration(img_pth0, img_pth1):\n",
    "    task = 'Single_Image_Defocus_Deblurring'  # Specify the restoration task\n",
    "\n",
    "    shutil.rmtree('demo')\n",
    "    input_dir = f'demo/sample_images/{task}/degraded'\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    \n",
    "    shutil.copy(img_pth0, input_dir)\n",
    "    shutil.copy(img_pth1, input_dir)\n",
    "\n",
    "    def get_weights_and_parameters(task):\n",
    "        weights = ''\n",
    "        parameters = {'inp_channels':3, 'out_channels':3, 'dim':48, 'num_blocks':[4,6,6,8], 'num_refinement_blocks':4, 'heads':[1,2,4,8], 'ffn_expansion_factor':2.66, 'bias':False, 'LayerNorm_type':'WithBias', 'dual_pixel_task':False}\n",
    "\n",
    "        if task == 'Motion_Deblurring':\n",
    "            weights = os.path.join('Motion_Deblurring', 'pretrained_models', 'motion_deblurring.pth')\n",
    "        elif task == 'Single_Image_Defocus_Deblurring':\n",
    "            weights = os.path.join('Defocus_Deblurring', 'pretrained_models', 'single_image_defocus_deblurring.pth')\n",
    "        elif task == 'Deraining':\n",
    "            weights = os.path.join('Deraining', 'pretrained_models', 'deraining.pth')\n",
    "        elif task == 'Real_Denoising':\n",
    "            weights = os.path.join('Denoising', 'pretrained_models', 'real_denoising.pth')\n",
    "            parameters['LayerNorm_type'] = 'BiasFree'\n",
    "\n",
    "        return weights, parameters\n",
    "\n",
    "    weights, parameters = get_weights_and_parameters(task)\n",
    "\n",
    "    load_arch = run_path(os.path.join('basicsr', 'models', 'archs', 'restormer_arch.py'))\n",
    "    model = load_arch['Restormer'](**parameters)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load(weights, map_location=device)\n",
    "    model.load_state_dict(checkpoint['params'])\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    out_dir = f'demo/sample_images/{task}/restored'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    img_multiple_of = 8\n",
    "\n",
    "    print(f\"\\n==> Running {task} with weights {weights}\\n\")\n",
    "    with torch.no_grad():\n",
    "        files = natsorted(glob(os.path.join(input_dir, '*')))\n",
    "        for filepath in tqdm(files):\n",
    "            img = cv2.cvtColor(cv2.imread(filepath), cv2.COLOR_BGR2RGB)\n",
    "            input_ = torch.from_numpy(img).float().div(255.).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "            # Pad the input if not_multiple_of 8\n",
    "            h, w = input_.shape[2], input_.shape[3]\n",
    "            H, W = ((h + img_multiple_of) // img_multiple_of) * img_multiple_of, ((w + img_multiple_of) // img_multiple_of) * img_multiple_of\n",
    "            padh = H - h if h % img_multiple_of != 0 else 0\n",
    "            padw = W - w if w % img_multiple_of != 0 else 0\n",
    "\n",
    "            # Convert input_ to NumPy array\n",
    "            input_ = input_.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "            input_ = np.pad(input_, ((0, padh), (0, padw), (0, 0)), mode='reflect')\n",
    "            input_ = torch.from_numpy(input_).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "            restored = model(input_)\n",
    "            restored = torch.clamp(restored, 0, 1)\n",
    "\n",
    "            # Unpad the output\n",
    "            restored = restored[:, :, :h, :w]\n",
    "\n",
    "            restored = restored.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "            restored = img_as_ubyte(restored[0])\n",
    "\n",
    "            filename = os.path.split(filepath)[-1]\n",
    "            cv2.imwrite(os.path.join(out_dir, filename), cv2.cvtColor(restored, cv2.COLOR_RGB2BGR))    \n",
    "    \n",
    "    new_img_pth0 = os.path.join(out_dir, os.listdir(out_dir)[0])\n",
    "    new_img_pth1 = os.path.join(out_dir, os.listdir(out_dir)[1])\n",
    "    print(os.listdir(out_dir)[0] + ' RESTORED')\n",
    "    print(os.listdir(out_dir)[1] + ' RESTORED')\n",
    "    return new_img_pth0, new_img_pth1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'src' from '__main__' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bcamaster\\Documents\\College\\Cawu5\\Research Methodology in Computer Science\\Rstormer_with_LoFTR\\restormer_with_loftr.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/bcamaster/Documents/College/Cawu5/Research%20Methodology%20in%20Computer%20Science/Rstormer_with_LoFTR/restormer_with_loftr.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimc_metric\u001b[39;00m \u001b[39mimport\u001b[39;00m EvaluateSubmission, ReadCovisibilityData, FlattenMatrix\n",
      "File \u001b[1;32mc:\\Users\\bcamaster\\Documents\\College\\Cawu5\\Research Methodology in Computer Science\\Rstormer_with_LoFTR\\imc_metric.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__main__\u001b[39;00m \u001b[39mimport\u001b[39;00m src\n\u001b[0;32m     10\u001b[0m Gt \u001b[39m=\u001b[39m namedtuple(\u001b[39m'\u001b[39m\u001b[39mGt\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39m'\u001b[39m\u001b[39mK\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mR\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mT\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m eps \u001b[39m=\u001b[39m \u001b[39m1e-15\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'src' from '__main__' (unknown location)"
     ]
    }
   ],
   "source": [
    "from imc_metric import EvaluateSubmission, ReadCovisibilityData, FlattenMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 1/16\n",
      "Loading covisibility data for \"british_museum\"... kept 5 out of 14793 covisible pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Running Single_Image_Defocus_Deblurring with weights Defocus_Deblurring\\pretrained_models\\single_image_defocus_deblurring.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "max_num_pairs = 3\n",
    "\n",
    "sample_ids = []\n",
    "fund_matrices = []\n",
    "\n",
    "count = 0\n",
    "scalingLen = len(scaling_dict.keys())\n",
    "\n",
    "for scene in scaling_dict.keys():\n",
    "    count += 1\n",
    "    print('Folder ' + str(count) + '/' + str(scalingLen))\n",
    "\n",
    "    covisibility_dict, F_gt_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')\n",
    "\n",
    "    # Let's remove pairs with a covisibility below 0.1. Note that the keys are roughly sorted by difficulty, in increasing order, so let's also shuffle before subsampling.\n",
    "    # Neither matters for the purposes of this exercise, but let's prevent mistakes down the line.\n",
    "    pairs = list([key for key, covis in covisibility_dict.items() if covis >= 0.1])\n",
    "    random.shuffle(pairs)\n",
    "    n = len(pairs)\n",
    "    pairs = pairs[:max_num_pairs]\n",
    "    # print(pairs)\n",
    "    print(f'Loading covisibility data for \"{scene}\"... kept {len(pairs)} out of {n} covisible pairs')\n",
    "    \n",
    "    for pair in tqdm(pairs):\n",
    "        image_0_id, image_1_id = pair.split('-')\n",
    "        img_path0 = f'{src}/{scene}/images/{image_0_id}.jpg'\n",
    "        img_path1 = f'{src}/{scene}/images/{image_1_id}.jpg'\n",
    "\n",
    "        img_path0, img_path1 = restoration(img_path0, img_path1)\n",
    "\n",
    "        mkpts0, mkpts1 = match(img_path0, img_path1, matcher)\n",
    "        F = get_F_matrix(mkpts0, mkpts1)\n",
    "        F_str = FlattenMatrix(F)\n",
    "\n",
    "        sample_ids.append(f'phototourism;{scene};{pair}')\n",
    "        fund_matrices.append(F_str)\n",
    "        # break\n",
    "    # if count == 3:\n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-1.22018306e-07 -6.62613654e-06 3.99141598e-03 5.95987564e-06 6.07795062e-07 1.70037789e-03 -3.63871381e-03 -1.96591492e-03 -6.72505285e-02',\n",
       " '-2.41235234e-08 -1.91292170e-05 9.04115090e-03 2.05223475e-05 1.42363615e-06 -5.60514708e-03 -7.00121359e-03 3.50388819e-03 2.77994731e-02',\n",
       " '-6.18233235e-08 2.42036373e-05 -1.50568824e-02 -2.44320500e-05 -7.04826017e-08 8.90095838e-03 1.25910132e-02 -1.17638305e-02 2.76461161e+00',\n",
       " '-3.35638462e-07 -2.04919190e-06 3.02819507e-03 -3.05961487e-06 8.83777143e-07 1.05327479e-02 1.54757938e-03 -5.15400244e-03 -1.22016902e+00',\n",
       " '-6.62005729e-08 3.40665384e-06 -1.52117259e-03 2.54410587e-06 3.65849484e-07 -8.32188128e-03 -1.38267342e-03 6.18289724e-03 1.27656738e+00',\n",
       " '1.25325427e-06 3.39444482e-05 -2.22396330e-02 -3.48926299e-05 1.72261748e-06 1.22335080e-02 6.56876200e-03 -1.26199964e-02 5.57929896e+00',\n",
       " '-7.14273559e-09 -2.44063252e-06 4.84979726e-04 2.76639098e-06 -4.72855914e-06 -5.38587294e-03 -4.61227358e-04 6.68347797e-03 -2.34207011e-01',\n",
       " '9.83380447e-08 -1.43479163e-06 4.37995516e-04 -3.46468780e-06 8.92062714e-08 7.43331873e-03 8.92554056e-04 -4.83075984e-03 2.63534314e-01',\n",
       " '1.82178853e-08 6.14440907e-06 -1.81829879e-03 -4.35169214e-06 -1.27757779e-06 7.68435628e-03 1.59879976e-03 -8.31993507e-03 -2.66004468e-01']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fund_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'sample_id':sample_ids, 'fundamental_matrix':fund_matrices})\n",
    "df.to_csv('train_pred_nores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluate prediction ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 1124.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene \"british_museum\" (3 pairs), mAA=0.06667\n",
      "Scene \"brandenburg_gate\" (3 pairs), mAA=0.00000\n",
      "Scene \"buckingham_palace\" (3 pairs), mAA=0.00000\n",
      "\n",
      "Full dataset: mAA=0.02222\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "thresholds_q = np.linspace(1, 10, 10)\n",
    "thresholds_t = np.geomspace(0.2, 5, 10)\n",
    "\n",
    "print('--- Evaluate prediction ---')\n",
    "maa, maa_per_scene, errors_dict_q, errors_dict_t = EvaluateSubmission('train_pred_nores.csv', scaling_dict, thresholds_q, thresholds_t)\n",
    "for scene, cur_maa in maa_per_scene.items():\n",
    "    print(f'Scene \"{scene}\" ({len(errors_dict_q[scene])} pairs), mAA={cur_maa:.05f}')\n",
    "print()\n",
    "print(f'Full dataset: mAA={maa:.05f}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
