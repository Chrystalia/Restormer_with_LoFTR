{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: imutils in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opencv-python) (1.23.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.23.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.23.2)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (9.2.0)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (2.31.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=21->scikit-image) (3.0.9)\n",
      "Requirement already satisfied: natsort in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (8.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.23.2)\n",
      "Requirement already satisfied: kornia in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.4)\n",
      "Requirement already satisfied: torch>=1.8.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kornia) (2.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kornia) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.1->kornia) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.1->kornia) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.1->kornia) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.1->kornia) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.8.1->kornia) (3.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->kornia) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.8.1->kornia) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.8.1->kornia) (1.3.0)\n",
      "Requirement already satisfied: torch in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: kornia in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement moons (from versions: none)\n",
      "ERROR: No matching distribution found for moons\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (1.23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchvision) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch==2.0.0->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bcamaster\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install einops\n",
    "! pip install imutils\n",
    "! pip install opencv-python\n",
    "! pip install matplotlib\n",
    "! pip install scikit-image\n",
    "! pip install natsort\n",
    "! pip install numpy\n",
    "! pip install kornia\n",
    "! pip install torch\n",
    "! pip install kornia moons\n",
    "! pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import namedtuple\n",
    "import cv2\n",
    "import csv\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from kornia.feature.loftr import LoFTR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "import imutils\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from kornia_moons.feature import *\n",
    "\n",
    "# from src.loftr import LoFTR, default_cfg\n",
    "from src.utils.plotting import make_matching_figure\n",
    "\n",
    "# =============================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from runpy import run_path\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "from numpy.lib.arraypad import pad\n",
    "\n",
    "# =================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "from glob import glob\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import kornia\n",
    "from kornia_moons.feature import *\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import gc\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "matcher = KF.LoFTR(pretrained=None)\n",
    "matcher.load_state_dict(torch.load(\"weights/outdoor_ds.ckpt\")['state_dict'])\n",
    "matcher = matcher.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "LONGEST_EDGE = 1280\n",
    "def resize_keep_ratio(img, longest_size=LONGEST_EDGE):\n",
    "    height, width = img.shape[:2]\n",
    "    if np.maximum(height, width) <= longest_size: # no need to resize\n",
    "        return img\n",
    "    \n",
    "    if height >= width:\n",
    "        resized_img = imutils.resize(img, height=longest_size)\n",
    "    else:\n",
    "        resized_img = imutils.resize(img, width=longest_size)\n",
    "    return resized_img\n",
    "\n",
    "def load_torch_image(fname, device):\n",
    "    img = cv2.imread(fname)\n",
    "    scale = 840 / max(img.shape[0], img.shape[1]) \n",
    "    w = int(img.shape[1] * scale)\n",
    "    h = int(img.shape[0] * scale)\n",
    "    img = cv2.resize(img, (w, h))\n",
    "    img = K.image_to_tensor(img, False).float() /255.\n",
    "    img = K.color.bgr_to_rgb(img)\n",
    "    return img.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(img_path0, img_path1, matcher, device=device):\n",
    "    img0 = load_torch_image(img_path0)\n",
    "    img1 = load_torch_image(img_path1)\n",
    "        \n",
    "    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0).to(device), \n",
    "                  \"image1\": K.color.rgb_to_grayscale(img1).to(device)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correspondences = matcher(input_dict)\n",
    "        \n",
    "    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n",
    "    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n",
    "        \n",
    "    return mkpts0, mkpts1\n",
    "        \n",
    "def get_F_matrix(mkpts0, mkpts1):\n",
    "\n",
    "    # Make sure we do not trigger an exception here.\n",
    "    if len(mkpts0) > 8:\n",
    "        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\n",
    "\n",
    "        assert F.shape == (3, 3), 'Malformed F?'\n",
    "    else:\n",
    "        F = np.zeros((3, 3))\n",
    "\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling factors: {'british_museum': 2.517, 'brandenburg_gate': 7.38, 'buckingham_palace': 18.75, 'colosseum_exterior': 36.99, 'grand_place_brussels': 10.26, 'lincoln_memorial_statue': 1.85, 'notre_dame_front_facade': 1.36, 'pantheon_exterior': 5.41, 'piazza_san_marco': 7.92, 'sacre_coeur': 20.27, 'sagrada_familia': 4.2, 'st_pauls_cathedral': 7.01, 'st_peters_square': 21.48, 'taj_mahal': 20.76, 'temple_nara_japan': 7.79, 'trevi_fountain': 3.67}\n"
     ]
    }
   ],
   "source": [
    "src = '../image-matching-challenge-2022/train'\n",
    "\n",
    "scaling_dict = {}\n",
    "with open(f'{src}/scaling_factors.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for i, row in enumerate(reader):\n",
    "        # Skip header.\n",
    "        if i == 0:\n",
    "            continue\n",
    "        scaling_dict[row[0]] = float(row[1])\n",
    "\n",
    "print(f'Scaling factors: {scaling_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restoration(img_pth0, img_pth1):\n",
    "    # task = 'Real_Denoising'\n",
    "    task = 'Single_Image_Defocus_Deblurring'\n",
    "    # task = 'Motion_Deblurring'\n",
    "    # task = 'Deraining'\n",
    "\n",
    "    shutil.rmtree('demo')\n",
    "\n",
    "    input_dir = 'demo/sample_images/'+task+'/degraded'\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    \n",
    "    shutil.copy(img_pth0, input_dir)\n",
    "    shutil.copy(img_pth1, input_dir)\n",
    "\n",
    "    def get_weights_and_parameters(task, parameters):\n",
    "        if task == 'Motion_Deblurring':\n",
    "            weights = os.path.join('Motion_Deblurring', 'pretrained_models', 'motion_deblurring.pth')\n",
    "        elif task == 'Single_Image_Defocus_Deblurring':\n",
    "            weights = os.path.join('Defocus_Deblurring', 'pretrained_models', 'single_image_defocus_deblurring.pth')\n",
    "        elif task == 'Deraining':\n",
    "            weights = os.path.join('Deraining', 'pretrained_models', 'deraining.pth')\n",
    "        elif task == 'Real_Denoising':\n",
    "            weights = os.path.join('Denoising', 'pretrained_models', 'real_denoising.pth')\n",
    "            parameters['LayerNorm_type'] =  'BiasFree'\n",
    "        return weights, parameters\n",
    "\n",
    "    # Get model weights and parameters\n",
    "    parameters = {'inp_channels':3, 'out_channels':3, 'dim':48, 'num_blocks':[4,6,6,8], 'num_refinement_blocks':4, 'heads':[1,2,4,8], 'ffn_expansion_factor':2.66, 'bias':False, 'LayerNorm_type':'WithBias', 'dual_pixel_task':False}\n",
    "    weights, parameters = get_weights_and_parameters(task, parameters)\n",
    "\n",
    "    load_arch = run_path(os.path.join('basicsr', 'models', 'archs', 'restormer_arch.py'))\n",
    "    model = load_arch['Restormer'](**parameters)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "    # print(device)\n",
    "    checkpoint = torch.load(weights)\n",
    "    # print(checkpoint)\n",
    "    model.load_state_dict(checkpoint['params'])\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    input_dir = 'demo/sample_images/'+task+'/degraded'\n",
    "    out_dir = 'demo/sample_images/'+task+'/restored'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    extensions = ['jpg', 'JPG', 'png', 'PNG', 'jpeg', 'JPEG', 'bmp', 'BMP']\n",
    "    files = natsorted(glob(os.path.join(input_dir, '*')))\n",
    "\n",
    "    img_multiple_of = 8\n",
    "\n",
    "    print(f\"\\n ==> Running {task} with weights {weights}\\n \")\n",
    "    with torch.no_grad():\n",
    "        for filepath in tqdm(files):\n",
    "            img = cv2.cvtColor(cv2.imread(filepath), cv2.COLOR_BGR2RGB)\n",
    "            input_ = torch.from_numpy(img).float().div(255.).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "            # Pad the input if not_multiple_of 8\n",
    "            h, w = input_.shape[2], input_.shape[3]\n",
    "            H, W = ((h + img_multiple_of) // img_multiple_of) * img_multiple_of, ((w + img_multiple_of) // img_multiple_of) * img_multiple_of\n",
    "            padh = H - h if h % img_multiple_of != 0 else 0\n",
    "            padw = W - w if w % img_multiple_of != 0 else 0\n",
    "\n",
    "            # Convert input_ to NumPy array\n",
    "            input_ = input_.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "            input_ = np.pad(input_, ((0, padh), (0, padw), (0, 0)), mode='reflect')\n",
    "            input_ = torch.from_numpy(input_).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "            restored = model(input_)\n",
    "            restored = torch.clamp(restored, 0, 1)\n",
    "\n",
    "            # Unpad the output\n",
    "            restored = restored[:, :, :h, :w]\n",
    "\n",
    "            restored = restored.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "            restored = img_as_ubyte(restored[0])\n",
    "\n",
    "            filename = os.path.split(filepath)[-1]\n",
    "            cv2.imwrite(os.path.join(out_dir, filename), cv2.cvtColor(restored, cv2.COLOR_RGB2BGR))    \n",
    "    \n",
    "    new_img_pth0 = out_dir + '/' + os.listdir(out_dir)[0]\n",
    "    new_img_pth1 = out_dir + '/' + os.listdir(out_dir)[1]\n",
    "    print(os.listdir(out_dir)[0] + ' RESTORED')\n",
    "    print(os.listdir(out_dir)[1] + ' RESTORED')\n",
    "    return new_img_pth0, new_img_pth1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imc_metric import EvaluateSubmission, ReadCovisibilityData, FlattenMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_num_pairs = 2\n",
    "\n",
    "# sample_ids = []\n",
    "# fund_matrices = []\n",
    "\n",
    "# count = 0\n",
    "# scalingLen = len(scaling_dict.keys())\n",
    "\n",
    "# for scene in scaling_dict.keys():\n",
    "#     # print(scene)\n",
    "#     count += 1\n",
    "#     print('Folder ' + str(count) + '/' + str(scalingLen))\n",
    "\n",
    "#     covisibility_dict, F_gt_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')\n",
    "\n",
    "#     # Let's remove pairs with a covisibility below 0.1. Note that the keys are roughly sorted by difficulty, in increasing order, so let's also shuffle before subsampling.\n",
    "#     # Neither matters for the purposes of this exercise, but let's prevent mistakes down the line.\n",
    "#     pairs = list([key for key, covis in covisibility_dict.items() if covis >= 0.7])\n",
    "#     random.shuffle(pairs)\n",
    "#     n = len(pairs)\n",
    "#     pairs = pairs[:max_num_pairs]\n",
    "#     # print(pairs)\n",
    "#     print(f'Loading covisibility data for \"{scene}\"... kept {len(pairs)} out of {n} covisible pairs')\n",
    "    \n",
    "#     for pair in tqdm(pairs):\n",
    "#         image_0_id, image_1_id = pair.split('-')\n",
    "#         img_path0 = f'{src}/{scene}/images/{image_0_id}.jpg'\n",
    "#         img_path1 = f'{src}/{scene}/images/{image_1_id}.jpg'\n",
    "\n",
    "#         # img_path0, img_path1 = restoration(img_path0, img_path1)\n",
    "\n",
    "#         mkpts0, mkpts1 = match(img_path0, img_path1, matcher)\n",
    "#         F = get_F_matrix(mkpts0, mkpts1)\n",
    "#         F_str = FlattenMatrix(F)\n",
    "\n",
    "#         sample_ids.append(f'phototourism;{scene};{pair}')\n",
    "#         fund_matrices.append(F_str)\n",
    "#         # break\n",
    "#     if count == 2:\n",
    "#         break\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 1/16\n",
      "Loading covisibility data for \"british_museum\"... kept 2 out of 102 covisible pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93658023_4980549800.jpg\n",
      "57504314_2114264842.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:09<00:09,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 3587\n",
      "77723525_5227836172.jpg\n",
      "57504314_2114264842.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:18<00:00,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 2631\n",
      "Folder 2/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading covisibility data for \"brandenburg_gate\"... kept 2 out of 750 covisible pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66720928_3418295149.jpg\n",
      "05712502_4051278060.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:17<00:17, 17.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 2785\n",
      "90920828_5082887495.jpg\n",
      "17262282_1141017004.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:36<00:00, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 3535\n",
      "Folder 3/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading covisibility data for \"buckingham_palace\"... kept 2 out of 49 covisible pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42572558_3847795752.jpg\n",
      "23458590_2731063123.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bcamaster\\Documents\\College\\Cawu5\\Research Methodology in Computer Science\\Restormer_with_LoFTR\\restormer_with_loftr.ipynb Cell 11\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bcamaster/Documents/College/Cawu5/Research%20Methodology%20in%20Computer%20Science/Restormer_with_LoFTR/restormer_with_loftr.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m input_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mimage0\u001b[39m\u001b[39m\"\u001b[39m: K\u001b[39m.\u001b[39mcolor\u001b[39m.\u001b[39mrgb_to_grayscale(image_0), \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bcamaster/Documents/College/Cawu5/Research%20Methodology%20in%20Computer%20Science/Restormer_with_LoFTR/restormer_with_loftr.ipynb#X15sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimage1\u001b[39m\u001b[39m\"\u001b[39m: K\u001b[39m.\u001b[39mcolor\u001b[39m.\u001b[39mrgb_to_grayscale(image_1)}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bcamaster/Documents/College/Cawu5/Research%20Methodology%20in%20Computer%20Science/Restormer_with_LoFTR/restormer_with_loftr.ipynb#X15sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/bcamaster/Documents/College/Cawu5/Research%20Methodology%20in%20Computer%20Science/Restormer_with_LoFTR/restormer_with_loftr.ipynb#X15sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     correspondences \u001b[39m=\u001b[39m matcher(input_dict)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bcamaster/Documents/College/Cawu5/Research%20Methodology%20in%20Computer%20Science/Restormer_with_LoFTR/restormer_with_loftr.ipynb#X15sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m mkpts0 \u001b[39m=\u001b[39m correspondences[\u001b[39m'\u001b[39m\u001b[39mkeypoints0\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bcamaster/Documents/College/Cawu5/Research%20Methodology%20in%20Computer%20Science/Restormer_with_LoFTR/restormer_with_loftr.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m mkpts1 \u001b[39m=\u001b[39m correspondences[\u001b[39m'\u001b[39m\u001b[39mkeypoints1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\bcamaster\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bcamaster\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kornia\\feature\\loftr\\loftr.py:164\u001b[0m, in \u001b[0;36mLoFTR.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    162\u001b[0m feat_f0_unfold, feat_f1_unfold \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfine_preprocess(feat_f0, feat_f1, feat_c0, feat_c1, data)\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m feat_f0_unfold\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# at least one coarse level predicted\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     feat_f0_unfold, feat_f1_unfold \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloftr_fine(feat_f0_unfold, feat_f1_unfold)\n\u001b[0;32m    166\u001b[0m \u001b[39m# 5. match fine-level\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfine_matching(feat_f0_unfold, feat_f1_unfold, data)\n",
      "File \u001b[1;32mc:\\Users\\bcamaster\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bcamaster\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kornia\\feature\\loftr\\loftr_module\\transformer.py:106\u001b[0m, in \u001b[0;36mLocalFeatureTransformer.forward\u001b[1;34m(self, feat0, feat1, mask0, mask1)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcross\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    105\u001b[0m     feat0 \u001b[39m=\u001b[39m layer(feat0, feat1, mask0, mask1)\n\u001b[1;32m--> 106\u001b[0m     feat1 \u001b[39m=\u001b[39m layer(feat1, feat0, mask1, mask0)\n\u001b[0;32m    107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bcamaster\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bcamaster\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kornia\\feature\\loftr\\loftr_module\\transformer.py:54\u001b[0m, in \u001b[0;36mLoFTREncoderLayer.forward\u001b[1;34m(self, x, source, x_mask, source_mask)\u001b[0m\n\u001b[0;32m     51\u001b[0m query, key, value \u001b[39m=\u001b[39m x, source, source\n\u001b[0;32m     53\u001b[0m \u001b[39m# multi-head attention\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_proj(query)\u001b[39m.\u001b[39mview(bs, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnhead, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim)  \u001b[39m# [N, L, (H, D)]\u001b[39;00m\n\u001b[0;32m     55\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(key)\u001b[39m.\u001b[39mview(bs, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnhead, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim)  \u001b[39m# [N, S, (H, D)]\u001b[39;00m\n\u001b[0;32m     56\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj(value)\u001b[39m.\u001b[39mview(bs, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnhead, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim)\n",
      "File \u001b[1;32mc:\\Users\\bcamaster\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bcamaster\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_num_pairs = 2\n",
    "\n",
    "F_dict = {}\n",
    "\n",
    "sample_ids = []\n",
    "fund_matrices = []\n",
    "\n",
    "count = 0\n",
    "folder_limit = 16\n",
    "scalingLen = len(scaling_dict.keys())\n",
    "\n",
    "import time\n",
    "for scene in scaling_dict.keys():\n",
    "    count += 1\n",
    "    print('Folder ' + str(count) + '/' + str(scalingLen))\n",
    "\n",
    "    covisibility_dict, F_gt_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')\n",
    "\n",
    "    # Let's remove pairs with a covisibility below 0.1. Note that the keys are roughly sorted by difficulty, in increasing order, so let's also shuffle before subsampling.\n",
    "    # Neither matters for the purposes of this exercise, but let's prevent mistakes down the line.\n",
    "    pairs = list([key for key, covis in covisibility_dict.items() if covis >= 0.7])\n",
    "    random.shuffle(pairs)\n",
    "    n = len(pairs)\n",
    "    pairs = pairs[:max_num_pairs]\n",
    "    print(f'Loading covisibility data for \"{scene}\"... kept {len(pairs)} out of {n} covisible pairs')\n",
    "\n",
    "    # Load the images.\n",
    "    for pair in tqdm(pairs):\n",
    "        st = time.time()\n",
    "        image_0_id, image_1_id = pair.split('-')\n",
    "        img_path0 = f'{src}/{scene}/images/{image_0_id}.jpg'\n",
    "        img_path1 = f'{src}/{scene}/images/{image_1_id}.jpg'\n",
    "\n",
    "        # img_path0, img_path1 = restoration(img_path0, img_path1)\n",
    "\n",
    "        # print(f'{image_0_id}.jpg')\n",
    "        # print(f'{image_1_id}.jpg')\n",
    "\n",
    "        image_0 = load_torch_image(img_path0, device)\n",
    "        image_1 = load_torch_image(img_path1, device)\n",
    "\n",
    "        input_dict = {\"image0\": K.color.rgb_to_grayscale(image_0), \n",
    "                \"image1\": K.color.rgb_to_grayscale(image_1)}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correspondences = matcher(input_dict)\n",
    "            \n",
    "        mkpts0 = correspondences['keypoints0'].cpu().numpy()\n",
    "        mkpts1 = correspondences['keypoints1'].cpu().numpy()\n",
    "        \n",
    "        if len(mkpts0) > 7:\n",
    "            #F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.25, 0.9999, 100000)\n",
    "            F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.1845, 0.999999, 220000)\n",
    "            inliers = inliers > 0\n",
    "            assert F.shape == (3, 3), 'Malformed F?'\n",
    "            # F_dict[sample_id] = F\n",
    "        else:\n",
    "            # F_dict[sample_id] = np.zeros((3, 3))\n",
    "            continue\n",
    "\n",
    "        F_str = FlattenMatrix(F)\n",
    "        sample_ids.append(f'phototourism;{scene};{pair}')\n",
    "        fund_matrices.append(F_str)\n",
    "\n",
    "        gc.collect()\n",
    "        nd = time.time()\n",
    "        print('Matches: {}'.format(len(mkpts0)))\n",
    "\n",
    "    if count == folder_limit:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8.09659935e-08 3.83725163e-06 -1.68533065e-03 -5.10271305e-06 -2.32134495e-07 6.33474070e-03 2.42883175e-03 -6.35963207e-03 -3.40481219e-01',\n",
       " '2.72286005e-06 -1.71651716e-04 4.10358635e-02 1.73362759e-04 -9.96309453e-07 -1.33780179e-01 -4.40917315e-02 1.31653566e-01 1.00000000e+00',\n",
       " '2.32940682e-09 -1.04529046e-05 4.36834589e-03 1.05961579e-05 -1.66145650e-07 2.99953328e-04 -5.18184729e-03 -5.67965389e-04 1.24622482e-01',\n",
       " '6.91690150e-09 1.64830708e-06 -6.68069937e-04 -6.11293678e-08 1.39626628e-07 4.43462089e-03 3.41584267e-06 -5.14727169e-03 2.65755199e-01',\n",
       " '-3.31288429e-07 1.27240775e-05 -5.93650056e-03 -1.27419800e-05 -8.85773132e-07 9.82647537e-03 5.96338569e-03 -8.30647196e-03 -3.62606427e-01',\n",
       " '1.81488361e-08 1.81461692e-05 -9.20610804e-03 -1.79502077e-05 -1.73929856e-07 8.73613601e-03 8.07882112e-03 -8.68600412e-03 5.10308514e-01']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fund_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phototourism;british_museum;77723525_5227836172-42803655_6429621523',\n",
       " 'phototourism;british_museum;19254317_158538587-18486676_4996206525',\n",
       " 'phototourism;british_museum;93658023_4980549800-42803655_6429621523',\n",
       " 'phototourism;british_museum;93658023_4980549800-77723525_5227836172',\n",
       " 'phototourism;british_museum;77723525_5227836172-69960354_6519615529',\n",
       " 'phototourism;british_museum;93658023_4980549800-75879177_2453112255']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'sample_id':sample_ids, 'fundamental_matrix':fund_matrices})\n",
    "df.to_csv('train_pred_nores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluate prediction ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 5856.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene \"british_museum\" (6 pairs), mAA=0.13333\n",
      "\n",
      "Full dataset: mAA=0.13333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "thresholds_q = np.linspace(1, 10, 10)\n",
    "thresholds_t = np.geomspace(0.2, 5, 10)\n",
    "\n",
    "print('--- Evaluate prediction ---')\n",
    "maa, maa_per_scene, errors_dict_q, errors_dict_t = EvaluateSubmission('train_pred_nores.csv', scaling_dict, thresholds_q, thresholds_t)\n",
    "for scene, cur_maa in maa_per_scene.items():\n",
    "    print(f'Scene \"{scene}\" ({len(errors_dict_q[scene])} pairs), mAA={cur_maa:.05f}')\n",
    "print()\n",
    "print(f'Full dataset: mAA={maa:.05f}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
